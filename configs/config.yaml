# Refer to Hydra documentation for more information about config group defaults.
# - https://hydra.cc/docs/tutorials/basic/your_first_app/defaults/
# - https://hydra.cc/docs/patterns/configuring_experiments/

defaults:
  - datasets: cifar10
  - models: resnet50
  - schedules: 10e
  - _self_
  # - override hydra/hydra_logging: none  # disable hydra logging
  # - override hydra/job_logging: none

# hydra:  # disable file output of hydra
#   run:
#     dir: .
#   output_subdir: null

hydra:
  mode: MULTIRUN  # refer to https://github.com/Lightning-AI/lightning/pull/11617
  sweep:
    dir: outputs/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}

trainer:
  # num_nodes: 1
  devices: 4
  accelerator: gpu
  strategy: ddp
  sync_batchnorm: True
  precision: 16

  # Refer to https://lightning.ai/docs/pytorch/latest/common/trainer.html for more infomation.
  # check_val_every_n_epoch: 1
  # log_every_n_steps: 50
  # enable_progress_bar: False
  # profiler: simple  # profiling measures the time consuming of all components

  # TODO: Build callbacks before passed to trainer.__init__().
  # callbacks:
  #   GradientAccumulationScheduler(scheduling={4: 2})

# Refer to
# - https://hydra.cc/docs/tutorials/basic/your_first_app/defaults/
# - https://hydra.cc/docs/patterns/configuring_experiments/
# for more information about config group defaults
defaults:
  - datasets: cifar10
  - models: resnet50
  - schedules: 10e
  - _self_
  # - override hydra/hydra_logging: none  # disable hydra logging
  # - override hydra/job_logging: none

# hydra:  # disable file output of hydra
#   run:
#     dir: .
#   output_subdir: null

hydra:
  mode: MULTIRUN  # otherwise the DDP output files may be in a mess, see https://github.com/Lightning-AI/lightning/pull/11617
  sweep:
    dir: outputs/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}

trainer:
  # num_nodes: 1
  devices: 4
  accelerator: gpu
  strategy: ddp  # ddp_find_unused_parameters_false is experimental and subject to change
  sync_batchnorm: True
  precision: 16

  # [OPTINAL]
  # The following are some flags may be useful for your further development.
  # Refer to https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html for more infomation.

  # enable_checkpointing: True
  # check_val_every_n_epoch: 1
  # accumulate_grad_batches: 1
  # log_every_n_steps: 50
  # enable_progress_bar: False
  # profiler: simple  # profiling measures the time consuming of all components

  # TODO: Build the callbacks before passed to trainer.__init__().
  # callbacks:
  #   LearningRateMonitor(logging_interval='step')
  #   ModelCheckpoint(dirpath='my/path/', filename='{epoch}-{val_loss:.2f}-{other_metric:.2f}')
  #   ModelSummary(max_depth=-1)
